import math
import random as rn
import warnings

import pandas as pd

pd.options.mode.chained_assignment = None
warnings.filterwarnings('ignore')

general_path = 'dataset/archive/Data'
gen = pd.read_csv(f'{general_path}/features_30_sec.csv')


class DataProcessing:
    @staticmethod
    def shuffle(x):
        for i in range(len(x) - 1, 0, -1):
            j = rn.randint(0, i - 1)
            x.iloc[i], x.iloc[j] = x.iloc[j], x.iloc[i]

    @staticmethod
    def min_max_scaling(x):
        values = x.select_dtypes(exclude=["object", "string"])
        columnNames = values.columns.tolist()
        for column in columnNames:
            x[column] = pd.to_numeric(x[column])
            data = x.loc[:, column]
            min1 = min(data)
            max1 = max(data)
            for row in range(len(x)):
                xprim = (x.at[row, column] - min1) / (max1 - min1)
                x.at[row, column] = xprim

    @staticmethod
    def split(x, train_ratio, val_ratio):
        train_size = int(len(x) * train_ratio)
        val_size = int(len(x) * val_ratio)
        test_size = len(x) - train_size - val_size

        train_set = x[:train_size]
        val_set = x[train_size:train_size + val_size]
        test_set = x[train_size + val_size:]

        return train_set, val_set, test_set


class KNN:
    def __init__(self, k):
        self.k = k

    def clustering(self, lista, sample):
        distances = []
        for i in range(len(lista)):
            temp = 0
            for j in range(len(sample)):
                temp += pow((lista.iloc[i][j] - sample[j]), 2)
            distances.append(math.sqrt(temp))
        lista["distance"] = distances
        lista = lista.sort_values("distance")
        dictionary = dict()
        dictionary1 = {"blues": 0, "classical": 0, "country": 0, "disco": 0, "hiphop": 0, "jazz": 0, "metal": 0,
                       "pop": 0, "reggae": 0, "rock": 0}
        for i in range(self.k):
            dictionary1[lista.iloc[i]["label"]] += 1
        return max(dictionary1, key=dictionary1.get)

    def knn_predict(self, lista, lista_t):
        pred = []
        for i in range(len(lista_t)):
            name = self.clustering(lista, pd.to_numeric(lista_t.iloc[i]))
            pred.append(name)
        return pred

    def accuracy(self, prediction, lista_v):
        counter = 0
        for i in range(len(lista_v)):
            if prediction[i] == lista_v.iloc[i]:
                counter += 1
        return (counter / len(lista_v)) * 100


DataProcessing.shuffle(gen)
print("Shuffled")

train_set, val_set, test_set = DataProcessing.split(gen, 0.7, 0.2)
print("Split")

DataProcessing.min_max_scaling(train_set)
val_set = val_set.reset_index(drop=True)
DataProcessing.min_max_scaling(val_set)
test_set = test_set.reset_index(drop=True)
DataProcessing.min_max_scaling(test_set)
print("Normalized")
features_list = [
    'chroma_stft_var',
    'rms_var',
    'spectral_centroid_var',
    'spectral_bandwidth_var',
    'rolloff_var',
    'zero_crossing_rate_var',
    'harmony_var',
    'perceptr_var',
    'perceptr_mean'
]

k = [5]
for feature in features_list:
    train_set = train_set.drop(feature, axis=1)
    val_set = val_set.drop(feature, axis=1)
print("Features dropped")
for i in k:
    knn = KNN(k=i)
    predicted = knn.knn_predict(train_set.drop(['filename'], axis=1), val_set.drop(['filename', 'label'], axis=1))
    accuracy = knn.accuracy(predicted, val_set['label'])
    print(f"Accuracy after cleanup: {accuracy}, k={i}")


