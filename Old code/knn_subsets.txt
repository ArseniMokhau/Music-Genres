import pandas as pd
import random as rn
import numpy as np
import seaborn as sns
import math
import warnings
from collections import Counter
from sklearn.model_selection import train_test_split

pd.options.mode.chained_assignment = None
warnings.filterwarnings('ignore')

general_path = 'dataset/archive/Data'
gen = pd.read_csv(f'{general_path}/features_30_sec.csv')


class DataProcessing:
    @staticmethod
    def shuffle(x):
        for i in range(len(x) - 1, 0, -1):
            j = rn.randint(0, i - 1)
            x.iloc[i], x.iloc[j] = x.iloc[j], x.iloc[i]

    @staticmethod
    def min_max_scaling(x):
        values = x.select_dtypes(exclude=["object", "string"])
        columnNames = values.columns.tolist()
        for column in columnNames:
            x[column] = pd.to_numeric(x[column])
            data = x.loc[:, column]
            min1 = min(data)
            max1 = max(data)
            for row in range(len(x)):
                xprim = (x.at[row, column] - min1) / (max1 - min1)
                x.at[row, column] = xprim

    @staticmethod
    def split(x, train_ratio, val_ratio):
        train_size = int(len(x) * train_ratio)
        val_size = int(len(x) * val_ratio)
        test_size = len(x) - train_size - val_size

        train_set = x[:train_size]
        val_set = x[train_size:train_size + val_size]
        test_set = x[train_size + val_size:]

        return train_set, val_set, test_set


class KNN:
    def __init__(self, k):
        self.k = k

    def clustering(self, lista, sample):
        distances = []
        for i in range(len(lista)):
            temp = 0
            for j in range(len(sample)):
                temp += pow((lista.iloc[i][j] - sample[j]), 2)
            distances.append(math.sqrt(temp))
        lista["distance"] = distances
        lista = lista.sort_values("distance")
        dictionary = dict()
        dictionary1 = {"blues": 0, "classical": 0, "country": 0, "disco": 0, "hiphop": 0, "jazz": 0, "metal": 0,
                       "pop": 0, "reggae": 0, "rock": 0}
        for i in range(self.k):
            dictionary1[lista.iloc[i]["label"]] += 1
        return max(dictionary1, key=dictionary1.get)

    def knn_predict(self, lista, lista_t):
        pred = []
        for i in range(len(lista_t)):
            name = self.clustering(lista, pd.to_numeric(lista_t.iloc[i]))
            pred.append(name)
        return pred

    def accuracy(self, prediction, lista_v):
        counter = 0
        for i in range(len(lista_v)):
            if prediction[i] == lista_v.iloc[i]:
                counter += 1
        return (counter / len(lista_v)) * 100


DataProcessing.shuffle(gen)
print("Shuffled")

train_set, val_set, test_set = DataProcessing.split(gen, 0.7, 0.2)
print("Split")

val_set = val_set.reset_index(drop=True)
DataProcessing.min_max_scaling(val_set)
test_set = test_set.reset_index(drop=True)
DataProcessing.min_max_scaling(test_set)
print("Normalized")

# New code for splitting train set into subsets and evaluating accuracy

subset_acc_list = []  # List to store accuracies of subsets
subset_list = []  # List to store the subsets

while len(subset_list) < 5:
    DataProcessing.shuffle(train_set)
    print("Train set reshuffled")
    subset = train_set.sample(frac=0.2)  # Splitting off a part of train set as a subset
    remaining_train_set = pd.concat([train_set, subset]).drop_duplicates(keep=False)  # Remaining part of train set

    subset = subset.reset_index(drop=True)
    DataProcessing.min_max_scaling(subset)

    knn = KNN(k=5)
    predicted = knn.knn_predict(subset.drop('filename', axis=1), val_set.drop(['filename', 'label'], axis=1))
    accuracy = knn.accuracy(predicted, val_set['label'])

    if accuracy >= 80.0:
        subset_acc_list.append(accuracy)
        subset_list.append(subset)
        train_set = remaining_train_set
        print(f"Subset Accuracy: {accuracy}%")
    else:
        print("Subset accuracy below 80%. Retrying...")

# Perform KNN on test set using each train subset and return the most common prediction

predictions_list = []
knn = KNN(k=5)

for subset in subset_list:
    predicted = knn.knn_predict(subset.drop('filename', axis=1), test_set.drop(['filename', 'label'], axis=1))
    most_common_prediction = Counter(predicted).most_common(1)[0][0]  # Get the most common prediction
    predictions_list.append(most_common_prediction)

test_accuracy = knn.accuracy(predictions_list, test_set['label'])
print(f"Test Accuracy with Subset Predictions: {test_accuracy}%")
